\chapter{Gvde}
\section{Descrizione}
{\em Gvde} è una versione modificata di {\em vde} che utilizza memoria condivisa invece dei sockets per far comunicare le macchine virtuali connesse allo switch. Si è quindi ritenuto necessario confrontare la nuova versione con la precedente al fine di valutare l'entità di eventuali benefici dovuti alla nuova implementazione.
\section{Il sistema utilizzato}
Tutti i test sono stati eseguiti su una distribuzione debian stable (wheezy)\footnote{http://snapshot.debian.org/archive/debian/20140123T042010Z/} utilizzando per {\em gvde} e {\em libvdeplug} il codice fornito e per {\em vde} l'ultima versione disponibile sul repository ufficiale con la {\em libvdeplug} fornita dal sistema. I software utilizzati, tutti presi dai repository debian, sono stati:
\begin{itemize}
\item {\em Linux kernel} versione 3.2.0-4-686-pae (3.2.51-1).
\item {\em Qemu-kvm} versione 1.1.2.
\item {\em Libvdeplug} versione 2.3.2-4.
\item {\em Iperf} versione 2.0.5 (08 Jul 2010) pthreads, per le misurazioni dell'ampiezza di banda e dei pacchetti persi.
\item {\em Libpcap} versione 1.3.0-1.
\end{itemize}
Il computer utilizzato è munito di processore Intel i7 2600K (quadcore con hyperthreading e vanderpool) e 8 GB di memoria ram.
\section{Gvde vs vde}
La principale differenza tra {\em gvde} e {\em vde} sta nella gestione dello smistamento dei pacchetti: in {\em vde} lo switch è incaricato di ricevere ed inviare i dati alle macchine virtuali, partecipando quindi attivamente al trasferimento dei dati; in {\em gvde} lo smistamento viene effettuato direttamente da {\em libvdeplug} mediante memoria condivisa, quindi attraverso la libreria caricata dalla macchina virtuale, riducendo quindi l'utilizzo del processore da parte dello switch a zero.
\subsection{I test effettuati}
Sono state prese in esame le configurazioni composte da due, quattro, sei ed otto macchine virtuali {\em qemu-kvm}, comunicanti due a due tra di loro attraverso l'utility {\em iperf} pensata specificatamente per analizzare le prestazioni di rete: prima viene eseguito in modalità TCP per stimare l'ampiezza di banda $B$, poi in modalità UDP, che necessita come input la velocità d'invio, per analizzare la quantità di pacchetti persi. In particolare, per la modalità UDP è stato generato traffico in ambo le direzioni con ampiezza $ \frac{B}{2*\#VMs} $ per singolo flusso.
\subsection{I risultati}
Legenda:
\begin{itemize}
\item {\em \#VMs}: numero di istanze {\em qemu-kvm}.
\item {\em BANDA MASSIMA}: somma flussi due a due unidirezionali misurati con {\em iperf} in modalità TCP.
\item {\em PACKETS LOSS}: media delle percentuali di pacchetti persi segnalate da ogni istanza di {\em iperf} in modalità UDP bidirezionale.
\item {\em CPU KVM}: massimo utilizzo del processore tra tutte le istanze di {\em qemu-kvm}.
\item {\em CPU VDE}: utilizzo del processore di {\em vde}.
\end{itemize}
\begin{table}[h]
\begin{tabular}{|p{0.1\textwidth}|p{0.2\textwidth}|p{0.175\textwidth}|p{0.175\textwidth}|p{0.175\textwidth}|}
\hline
\#VMs    & BANDA MASSIMA       & PACKETS LOSS   & \%CPU KVM & \%CPU VDE \\
\hline
\hline
2       & 390Mbits/s         & 0.45\%   & 100\% & 24\% \\
\hline
4       & 540Mbits/s         & 0.37\%   & 90\% & 25\% \\
\hline
6       & 550Mbits/s         & 0.35\%  & 88\% & 26\% \\
\hline
8       & 555Mbits/s         & 0.20\%  & 76\% & 26\% \\
\hline
\end{tabular}
\caption{Vde}
\label{vde}
\end{table}
\begin{table}[h]
\begin{tabular}{|p{0.1\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|}
\hline
\#VMs    & BANDA MASSIMA       & PACKETS LOSS   & \%CPU KVM \\
\hline
\hline
2       & 420Mbits/s         & 2.4\%   & 100\% \\
\hline
4       & 560Mbits/s         & 0.65\%   & 100\% \\
\hline
6       & 720Mbits/s         & 0.77\%  & 90\% \\
\hline
8       & 720Mbits/s         & 1\%  & 77\% \\
\hline
\end{tabular}
\caption{Gvde}
\label{gvde}
\end{table}
\subsection{Analisi}
Come possiamo notare dalle tabelle {\ref{vde}} e {\ref{gvde}}, {\em gvde} si mostra più performante e scalabile, con una crescita di pacchetti persi proporzionale al numero di macchine virtuali connesse ed un guadagno consistente in termini di ampiezza di banda disponibile all'aumentare dei flussi di trasferimento fino al raggiungimento di una costante, la quale è stata raggiunta con sei flussi contemporanei generati da altrettante macchine. Per quanto riguarda {\em vde}, il limite di trasferimento viene già sfiorato con solo quattro istanze connesse e la percentuale di pacchetti persi sembra dipendere più dall'ampiezza del singolo flusso che dalla loro somma (una volta che questa ha raggiunto il limite), infatti la percentuale di pacchetti persi ha un andamento decrescente. Curioso notare una percentuale di pacchetti persi molto alta con {\em gvde} e solo due macchine virtuali, ripetibile sperimentalmente, che potrebbe indicare un limite massimo per la velocità di trasferimento per un singolo flusso in questa modalità.
\section{Vde\_pcapplug2}
{\em Vde\_pcapplug2} permette di connettere la rete virtuale gestita dallo switch ad un'interfaccia di rete e, quindi, ad una rete fisica esistente, trasmettendo tutto ciò che riceve da un capo della connessione all'altro, ovvero inoltrando tutto il traffico proveniente dallo switch sulla rete fisica e viceversa.
La particolarità di {\em vde\_pcapplug2} sta nel sfruttare socket mmappati per ricevere e trasmettere pacchetti invece di utilizzare la libreria libpcap, ovvero condividendo due porzioni di memoria con il kernel attraverso la syscall {\em mmap()}, le quali saranno trattate come due buffer circolari, uno per la ricezione ed uno per l'invio: il processo utente per inviare dati riempirà il buffer opportuno, contrassegnando di volta in volta le posizioni scritte in modo che il kernel autonomamente possa processarle e ricontrassegnarle come vuote; analogo funzionamento per la ricezione dove sarà il kernel a riempire il buffer e l'utente a leggere e liberare le posizioni. Il vantaggio di questa modalità è l'eliminazione delle syscalls {\em read()}, {\em write()} ed affini per ricevere ed inviare, in modo da ridurre i {\em context switches} per eseguire le varie syscalls.
\subsection{I test effettuati}
Sono stati effettuati test analizzando il traffico passante tra una macchina virtuale collegata a {\em gvde} ed una macchina fisica connessa direttamente via cavo all'interfaccia {\em eth1}, separata dalla rete principale per non aver interferenze sui dati raccolti, con i protocolli TCP ed UDP. Si è poi proceduto alla ricerca della causa dello strano comportamento di {\em vde\_pcapplug2}, effettuando modifiche al codice per controllare tempistiche di smistamento e modalità di funzionamento dei buffer circolari, ovvero modalità di visita del buffer e parametri di inizializzazione dei buffer stessi. Si è inoltre utilizzata anche l'utility {\em valgrind} per un controllo generale, ma sistematico, del codice.
\subsection{I risultati}
Legenda:
\begin{itemize}
\item {\em DIREZIONE}: con {\em IN} si intende che il flusso è verso la macchina con lo switch attivo, con {\em OUT} il contrario, con {\em IN/OUT} flusso bidirezionale.
\item {\em BANDA MASSIMA}: somma flussi misurati con {\em iperf} in modalità TCP.
\item {\em BANDA RICHIESTA}: velocità d'invio passata in input ad {\em iperf} in modalità UDP.
\item {\em PACKETS LOSS}: media delle percentuali di pacchetti persi segnalate da ogni istanza di {\em iperf} in modalità UDP.
\end{itemize}
\begin{table}[h]
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
\hline
DIREZIONE    & BANDA MASSIMA       \\
\hline
\hline
IN       &   17.1Mbits/s       \\
\hline
OUT       &   93.9Mbits/s       \\
\hline
IN/OUT      & 558Kbits/s - 93.8Mbits/s   \\
\hline
\end{tabular}
\caption{Vde\_pcapplug2 con flussi TCP (socket mmappati)}
\label{pcapplugtcp}
\end{table}
\begin{table}[h]
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
\hline
DIREZIONE    & BANDA MASSIMA       \\
\hline
\hline
IN       &   15.3Mbits/s       \\
\hline
OUT       &   94Mbits/s       \\
\hline
IN/OUT      & 1.1Mbits/s - 93.9Mbits/s   \\
\hline
\end{tabular}
\caption{Vde\_pcapplug2 con flussi TCP (libpcap)}
\label{pcapplugtcpx}
\end{table}
\begin{table}[h]
\begin{tabular}{|p{0.33\textwidth}|p{0.33\textwidth}|p{0.33\textwidth}|}
\hline
DIREZIONE    & BANDA RICHIESTA   &  PACKETS LOSS   \\
\hline
\hline
IN       & 50Mbits/s  &  0\%   \\
\hline
IN       & 95Mbits/s  &  0\%   \\
\hline
OUT      & 50Mbits/s  &  0\%  \\
\hline
OUT      & 95Mbits/s  &  0\%  \\
\hline
IN/OUT   & 50Mbits/s  &  0\% - 0\%   \\
\hline
IN/OUT   & 95Mbits/s  &  0.04\% - 0.005\%   \\
\hline
\end{tabular}
\caption{Vde\_pcapplug2 con flussi UDP (socket mmappati)}
\label{pcapplugudp}
\end{table}
\begin{table}[h]
\begin{tabular}{|p{0.33\textwidth}|p{0.33\textwidth}|p{0.33\textwidth}|}
\hline
DIREZIONE    & BANDA RICHIESTA   &  PACKETS LOSS   \\
\hline
\hline
IN       & 50Mbits/s  &  0\%   \\
\hline
IN       & 95Mbits/s  &  0\%   \\
\hline
OUT      & 50Mbits/s  &  0\%  \\
\hline
OUT      & 95Mbits/s  &  0\%  \\
\hline
IN/OUT   & 50Mbits/s  &  0\% - 0\%   \\
\hline
IN/OUT   & 95Mbits/s  &  0.02\% - 0\%   \\
\hline
\end{tabular}
\caption{Vde\_pcapplug2 con flussi UDP (libpcap)}
\label{pcapplugudpx}
\end{table}
\subsection{Analisi}
Come si può banalmente evincere dalle tabelle {\ref{pcapplugudp}} e {\ref{pcapplugudpx}}, non si tratta di un problema di perdita di pacchetti. Dai dati raccolti sperimentalmente si nota un comportamento quasi bipolare cambiando protocollo di trasporto: utilizzando UDP ed impostando manualmente la velocità di trasmissione, non ci sono problemi, ricezione ed invio sembrano funzionare, anche con flussi bidirezionali; con TCP {\em vde\_pcapplug2} pare comportarsi in modo inspiegabile, ovvero per quanto riguarda il traffico in uscita dallo switch verso la rete fisica non ci sono problemi mentre quello in entrata sembra essere rallentato, se non addirittura completamente compromesso in prestazioni se si utilizza un flusso bidirezionale. Questa stranezza comunque pare non essere causata da {\em vde\_pcapplug2}, poiché alla luce di modifiche al codice per osservare le modalità di lettura del buffer circolare dedicato al traffico in entrata e quantificare i tempi di invio allo switch dei dati ricevuti non si è trovato nulla di anomalo nel codice: se ad ogni risveglio del processo dopo la syscall {\em poll()} durante in trasferimento con protocollo TCP viene controllato l'intero buffer dedicato alla ricezione, si può notare come esso stia sempre praticamente vuoto, ovvero con una o due posizioni occupate al massimo, come se fosse proprio il kernel stesso a non rendere disponibili i pacchetti, perché ci si aspetterebbe, in caso di traffico lento, una lentezza nello svuotare il buffer oppure un problema di pacchetti persi, esclusi con l'analisi del traffico UDP.
\subsection{Possibili sviluppi}
In futuro sarebbe interessante provare ad integrare in {\em vde\_pcapplug2} il supporto al framework {\em netmap} \footnote{http://info.iet.unipi.it/$\sim$luigi/netmap/} di Luigi Rizzo dell'università di Pisa, utilizzato nello suo switch virtuale {\em VALE}, il quale è un framework dedicato all'I/O su rete ad alta velocità, basato però su un modulo kernel per migliorare l'efficienza di {\em linux} nella gestione di {\em packet capturing} a livello utente.